{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Introduction__Liu_version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4BqXpsePCoJ",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch 基本介紹（含 code demo）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saI99CYkP0IV",
        "colab_type": "text"
      },
      "source": [
        "[搭配投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML2020/PyTorch_Introduction.slides.html#/23)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2frzqLOlOdWt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@markdown （請先記得將 Colab 硬體 runtime 調成 GPU 模式喔！）  \n",
        "\n",
        "#@markdown 先下載等等需要的 data \n",
        "#@markdown ###（**_記得 run 我！_**）\n",
        "\n",
        "%%bash\n",
        "wget -q -N https://download.pytorch.org/tutorial/faces.zip\n",
        "if [ ! -d data ]; then mkdir data; fi\n",
        "unzip -q -o faces.zip -d data\n",
        "rm -f faces.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkG3w8wHPP5k",
        "colab_type": "text"
      },
      "source": [
        "#### 載入需要的 libraries （函式庫）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoZ3C2l_IMwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 深度學習套件\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 作圖套件\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 影像處理套件\n",
        "import torchvision.transforms as transforms\n",
        "from skimage import io, transform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L28XqVbKPl0Z",
        "colab_type": "text"
      },
      "source": [
        "◉ 為了可以 reproduce (重現)，最好固定住 random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8KXqG-9PjeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(446)\n",
        "np.random.seed(446)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQ8bSDYQQpy",
        "colab_type": "text"
      },
      "source": [
        "## Tensor（張量）和 Numpy array 的關係與互相轉換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmKLRtwgIEqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "6c0cf26d-e76f-4644-8b8f-8b860109cdbc"
      },
      "source": [
        "# we create tensors in a similar way to numpy nd arrays\n",
        "x_numpy = np.array([0.1, 0.2, 0.3])\n",
        "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
        "print('x_numpy, x_torch')\n",
        "print(x_numpy, x_torch)\n",
        "print()\n",
        "\n",
        "# to and from numpy, pytorch\n",
        "print('to and from numpy and pytorch')\n",
        "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
        "print()\n",
        "\n",
        "# we can do basic operations like +-*/\n",
        "y_numpy = np.array([3,4,5.])\n",
        "y_torch = torch.tensor([3,4,5.])\n",
        "print(\"x+y\")\n",
        "print(x_numpy + y_numpy, x_torch + y_torch)\n",
        "print()\n",
        "\n",
        "# many functions that are in numpy are also in pytorch\n",
        "print(\"norm\")\n",
        "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
        "print()\n",
        "\n",
        "# to apply an operation along a dimension,\n",
        "# we use the dim keyword argument instead of axis\n",
        "print(\"mean along the 0th dimension\")\n",
        "x_numpy = np.array([[1,2],[3,4.]])\n",
        "x_torch = torch.tensor([[1,2],[3,4.]])\n",
        "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_numpy, x_torch\n",
            "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n",
            "\n",
            "to and from numpy and pytorch\n",
            "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
            "\n",
            "x+y\n",
            "[3.1 4.2 5.3] tensor([3.1000, 4.2000, 5.3000])\n",
            "\n",
            "norm\n",
            "0.37416573867739417 tensor(0.3742)\n",
            "\n",
            "mean along the 0th dimension\n",
            "[2. 3.] tensor([2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5hYZ6HeQgUR",
        "colab_type": "text"
      },
      "source": [
        "### **`Tensor.view`**  \n",
        "改變 tensor 的 shape （形狀、各維度大小），類似於 `numpy.reshape()`  \n",
        "可以傳入一個 `-1` 表示不固定的維度（會根據資料數量計算，無法整除會出錯提醒）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd8E6JMEIUP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "ef796212-3c22-4950-d7f9-d09e35f82989"
      },
      "source": [
        "# \"MNIST\"\n",
        "N, C, W, H = 10000, 3, 28, 28\n",
        "X = torch.randn((N, C, W, H))\n",
        "\n",
        "print(X.shape)\n",
        "print(X.view(N, C, 784).shape)\n",
        "print(X.view(-1, C, 784).shape) # automatically choose the 0th dimension"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 3, 28, 28])\n",
            "torch.Size([10000, 3, 784])\n",
            "torch.Size([10000, 3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld6mll0uRL3q",
        "colab_type": "text"
      },
      "source": [
        "### Computation graphs （計算圖）\n",
        "將 tensor 的計算用 graph 表示  \n",
        "<img src=\"https://i.imgur.com/KuiL6qo.png\" height=\"320px\"/>  \n",
        "以下計算 $e=(a+b)\\times(b+1)$  \n",
        "其中 $a=2, b=1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeUc7uYmIWFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "abe87477-3f55-40ef-9b7d-128af5e80e79"
      },
      "source": [
        "a = torch.tensor(2.0, requires_grad=True) # we set requires_grad=True to let PyTorch know to keep the graph\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "c = a + b\n",
        "d = b + 1\n",
        "e = c * d\n",
        "print('c', c)\n",
        "print('d', d)\n",
        "print('e', e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c tensor(3., grad_fn=<AddBackward0>)\n",
            "d tensor(2., grad_fn=<AddBackward0>)\n",
            "e tensor(6., grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv606Nt3SLlD",
        "colab_type": "text"
      },
      "source": [
        "## 將 PyTorch 用作自動求導框架\n",
        "由於在進行深度學習時經常需要取 gradient （梯度、導函數、微分），以下示範對函數進行求導的過程\n",
        "\n",
        "考慮函數 $f(x) = (x-2)^2$\n",
        "我們來計算 $f(x)$ 對 $x$ 在 $x = 1$ 時的微分，也就是求 $f'(1)$\n",
        "\n",
        "使用 `backward()` 可以自動對 leaf variable（計算圖上的 leaf）取微分（詳見課程 **back propagation**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pEGE-8zIWSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "341e5e46-dbe5-4d9f-9149-398a01fa9c33"
      },
      "source": [
        "def f(x):\n",
        "    return (x-2)**2\n",
        "\n",
        "def fp(x):\n",
        "    return 2*(x-2)\n",
        "\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "y = f(x)\n",
        "y.backward()\n",
        "\n",
        "print('Analytical f\\'(x):', fp(x))\n",
        "print('PyTorch\\'s f\\'(x):', x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
            "PyTorch's f'(x): tensor([-2.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqYsOC7TMHj",
        "colab_type": "text"
      },
      "source": [
        "對於向量、矩陣和函數一樣可以求微分  \n",
        "設 $w = [w_1, w_2]^T$ ，考慮函數 $g(w)=2w_1w_2+w_2\\cos(w_1)$ ，計算 $\\nabla_wg(w)$ ，並驗證 $\\nabla_wg([\\pi,1])=[2,\\pi-1]^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO4LWWttIWW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "06d4eb8f-5a7a-45f4-9766-14db2f6d5b5c"
      },
      "source": [
        "def g(w):\n",
        "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
        "\n",
        "def grad_g(w):\n",
        "    return torch.tensor([2*w[1] - w[1]*torch.sin(w[0]), 2*w[0] + torch.cos(w[0])])\n",
        "\n",
        "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
        "\n",
        "z = g(w)\n",
        "z.backward()\n",
        "\n",
        "print('Analytical grad g(w)', grad_g(w))\n",
        "print('PyTorch\\'s grad g(w)', w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical grad g(w) tensor([2.0000, 5.2832])\n",
            "PyTorch's grad g(w) tensor([2.0000, 5.2832])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUrQbvxwT_jg",
        "colab_type": "text"
      },
      "source": [
        "使用梯度更新參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d03U-bN-IWaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "718f4470-028e-453c-92c8-7010e26ea254"
      },
      "source": [
        "x = torch.tensor([5.0], requires_grad=True)\n",
        "step_size = 0.25\n",
        "\n",
        "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
        "for i in range(15):\n",
        "    y = f(x)\n",
        "    y.backward() # compute the gradient\n",
        "    \n",
        "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
        "    \n",
        "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
        "    \n",
        "    # We need to zero the grad variable since the backward()\n",
        "    # call accumulates the gradients in .grad instead of overwriting.\n",
        "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
        "    x.grad.detach_()\n",
        "    x.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tx,\tf(x),\tf'(x),\tf'(x) pytorch\n",
            "0,\t5.000,\t9.000,\t6.000,\t6.000\n",
            "1,\t3.500,\t2.250,\t3.000,\t3.000\n",
            "2,\t2.750,\t0.562,\t1.500,\t1.500\n",
            "3,\t2.375,\t0.141,\t0.750,\t0.750\n",
            "4,\t2.188,\t0.035,\t0.375,\t0.375\n",
            "5,\t2.094,\t0.009,\t0.188,\t0.188\n",
            "6,\t2.047,\t0.002,\t0.094,\t0.094\n",
            "7,\t2.023,\t0.001,\t0.047,\t0.047\n",
            "8,\t2.012,\t0.000,\t0.023,\t0.023\n",
            "9,\t2.006,\t0.000,\t0.012,\t0.012\n",
            "10,\t2.003,\t0.000,\t0.006,\t0.006\n",
            "11,\t2.001,\t0.000,\t0.003,\t0.003\n",
            "12,\t2.001,\t0.000,\t0.001,\t0.001\n",
            "13,\t2.000,\t0.000,\t0.001,\t0.001\n",
            "14,\t2.000,\t0.000,\t0.000,\t0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx4M8agHUJAD",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression（線性迴歸）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO7mvW5KIWdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "9d42cbd6-773d-49fe-da28-426f6cfd9879"
      },
      "source": [
        "# make a simple linear dataset with some noise\n",
        "\n",
        "d = 2\n",
        "n = 50\n",
        "X = torch.randn(n,d)\n",
        "true_w = torch.tensor([[-1.0], [2.0]])\n",
        "y = X @ true_w + torch.randn(n,1) * 0.1\n",
        "print('X shape', X.shape)\n",
        "print('y shape', y.shape)\n",
        "print('w shape', true_w.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape torch.Size([50, 2])\n",
            "y shape torch.Size([50, 1])\n",
            "w shape torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bu9-fQHUTNN",
        "colab_type": "text"
      },
      "source": [
        "確認 PyTorch 計算的梯度是否和公式一致——（RSS: residual sum of squares）  \n",
        "$\\nabla_w\\mathcal{L}_{RSS}(w; X)=\\nabla_w\\cfrac{1}{n}\\left \\| y-Xw \\right \\|^2_2=-\\cfrac{2}{n}X^T(y-Xw)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INzlel7EIWhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "5da8575c-43b2-4c61-a1f6-b3104ce33542"
      },
      "source": [
        "# define a linear model with no bias\n",
        "def model(X, w):\n",
        "    return X @ w\n",
        "\n",
        "# the residual sum of squares loss function\n",
        "def rss(y, y_hat):\n",
        "    return torch.norm(y - y_hat)**2 / n\n",
        "\n",
        "# analytical expression for the gradient\n",
        "def grad_rss(X, y, w):\n",
        "    return -2*X.t() @ (y - X @ w) / n\n",
        "\n",
        "w = torch.tensor([[1.], [0]], requires_grad=True)\n",
        "y_hat = model(X, w)\n",
        "\n",
        "loss = rss(y, y_hat)\n",
        "loss.backward()\n",
        "\n",
        "print('Analytical gradient', grad_rss(X, y, w).detach().view(2).numpy())\n",
        "print('PyTorch\\'s gradient', w.grad.view(2).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical gradient [ 5.1867113 -5.5912566]\n",
            "PyTorch's gradient [ 5.186712  -5.5912566]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps-AL_lrVpo-",
        "colab_type": "text"
      },
      "source": [
        "試試看用用自動計算的 gradient （梯度）進行 **gradient descent（GD）** 更新 model（模型）參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTdLbCrZIWld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "be62987f-9ac5-4e35-e72a-1ab179339260"
      },
      "source": [
        "step_size = 0.1\n",
        "\n",
        "print('iter,\\tloss,\\tw')\n",
        "for i in range(20):\n",
        "    y_hat = model(X, w)\n",
        "    loss = rss(y, y_hat)\n",
        "    \n",
        "    loss.backward() # compute the gradient of the loss\n",
        "    \n",
        "    w.data = w.data - step_size * w.grad # do a gradient descent step\n",
        "    \n",
        "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), w.view(2).detach().numpy()))\n",
        "    \n",
        "    # We need to zero the grad variable since the backward()\n",
        "    # call accumulates the gradients in .grad instead of overwriting.\n",
        "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
        "    w.grad.detach()\n",
        "    w.grad.zero_()\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', w.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t10.80,\t[-0.03734243  1.1182513 ]\n",
            "1,\t2.31,\t[-0.28690195  1.3653738 ]\n",
            "2,\t1.24,\t[-0.4724271  1.5428905]\n",
            "3,\t0.67,\t[-0.6105486  1.6702049]\n",
            "4,\t0.36,\t[-0.71353513  1.7613506 ]\n",
            "5,\t0.20,\t[-0.79044634  1.8264704 ]\n",
            "6,\t0.11,\t[-0.8479796  1.8728881]\n",
            "7,\t0.06,\t[-0.89109135  1.9058872 ]\n",
            "8,\t0.04,\t[-0.92345405  1.9292755 ]\n",
            "9,\t0.03,\t[-0.94779253  1.9457937 ]\n",
            "10,\t0.02,\t[-0.9661309  1.957412 ]\n",
            "11,\t0.01,\t[-0.97997516  1.9655445 ]\n",
            "12,\t0.01,\t[-0.9904472  1.9712044]\n",
            "13,\t0.01,\t[-0.9983844  1.9751165]\n",
            "14,\t0.01,\t[-1.0044125  1.9777979]\n",
            "15,\t0.01,\t[-1.0090001  1.9796168]\n",
            "16,\t0.01,\t[-1.0124985  1.9808345]\n",
            "17,\t0.01,\t[-1.0151719  1.9816359]\n",
            "18,\t0.01,\t[-1.0172188  1.9821515]\n",
            "19,\t0.01,\t[-1.0187894  1.9824725]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-1.0187894  1.9824725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXk9TWYpbW99",
        "colab_type": "text"
      },
      "source": [
        "## 用 **`torch.nn.Module`** 搭一個 model\n",
        "常用的 neural network layers 都在 `torch.nn` 的 module 底下，  \n",
        "  而常見的函數則在 `torch.nn.functional` 下找得到，  \n",
        "不過 layer 跟 functional 的差異在於後者的參數需要自己處理  \n",
        "對於 model 的初始化（initialization）參數可以使用 `torch.nn.init` module\n",
        "\n",
        "**`nn.Linear()`** 就是最常見的 fully connected layer  \n",
        "\n",
        "<img src=\"https://i.imgur.com/Ilpvqaz.png\" height=\"300px\"/>  \n",
        "（取自李宏毅教授的影片  \n",
        "*ML Lecture 6: Brief Introduction of Deep Learning*  https://www.youtube.com/watch?v=Dr-WRlEFefw ）\n",
        "\n",
        "以下示範一個簡單的 Linear model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWYmMqsKIkYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0e3b895f-aacf-4a93-ed8f-33862946f84b"
      },
      "source": [
        "d_in = 3\n",
        "d_out = 4\n",
        "linear_module = nn.Linear(d_in, d_out)\n",
        "\n",
        "example_tensor = torch.tensor([[1.,2,3], [4,5,6]])\n",
        "# applys a linear transformation to the data\n",
        "transformed = linear_module(example_tensor)\n",
        "print('example_tensor', example_tensor.shape)\n",
        "print('transormed', transformed.shape)\n",
        "print()\n",
        "print('We can see that the weights exist in the background\\n')\n",
        "print('W:', linear_module.weight)\n",
        "print('b:', linear_module.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example_tensor torch.Size([2, 3])\n",
            "transormed torch.Size([2, 4])\n",
            "\n",
            "We can see that the weights exist in the background\n",
            "\n",
            "W: Parameter containing:\n",
            "tensor([[ 0.2151, -0.2631,  0.4498],\n",
            "        [-0.3092,  0.3098, -0.4239],\n",
            "        [-0.0499, -0.2222,  0.0085],\n",
            "        [-0.0356,  0.5260,  0.4925]], requires_grad=True)\n",
            "b: Parameter containing:\n",
            "tensor([-0.0887,  0.3944,  0.4080,  0.2182], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-w4Jyg8ee-9",
        "colab_type": "text"
      },
      "source": [
        "### Activation functions\n",
        "PyTorch 有實作常到用的 `ReLU`, `Tanh` 和 `Sigmoid`  \n",
        "如果是 `nn.ReLU` 之類的 modules 需要被 instantiated  \n",
        "如果是 `nn.functional`（常簡寫為 `F`）的 functions 則可以直接 apply\n",
        "\n",
        "> **Python modules (classes), objects 與 functions**  \n",
        "> 相信考過先備能力測驗的各位應該多少有些觀念，不過還是稍微提醒一下  \n",
        "> 像是 `nn.ReLU` 是個 class，需要經過 constructor（建構子 / 建構函數）創造一個 instance / object；  \n",
        "> 而 `F.relu` 則可以直接傳入變數使用，但一些 parameters 需要自己處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0R6s-zInK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "d3669ecb-7b69-4c64-9cb7-218e3c10b592"
      },
      "source": [
        "activation_fn = nn.ReLU() # we instantiate an instance of the ReLU module\n",
        "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
        "activated = activation_fn(example_tensor)\n",
        "print('example_tensor', example_tensor)\n",
        "print('activated', activated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example_tensor tensor([-1.,  1.,  0.])\n",
            "activated tensor([0., 1., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhB3FwPkFTrV",
        "colab_type": "text"
      },
      "source": [
        "### **`nn.Sequential`**\n",
        "`nn.Sequential` 可以將常用的模型架構（layer、function 等 module）組合起來，以便管理或重複使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl830qZeIpPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1b73123c-27b5-49c2-f86d-ced34fc20890"
      },
      "source": [
        "d_in = 3\n",
        "d_hidden = 4\n",
        "d_out = 1\n",
        "model = torch.nn.Sequential(\n",
        "                            nn.Linear(d_in, d_hidden),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(d_hidden, d_out),\n",
        "                            nn.Sigmoid()\n",
        "                           )\n",
        "\n",
        "example_tensor = torch.tensor([[1.,2,3],[4,5,6]])\n",
        "transformed = model(example_tensor)\n",
        "print('transformed', transformed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transformed torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTxLge2EGM8t",
        "colab_type": "text"
      },
      "source": [
        "model 的參數可以用 `model.parameters()` 取得  \n",
        "（實作時最常用在 optimizer 更新參數上）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmKUDushIsZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "04362f28-7e6a-4471-c73f-c6737054202c"
      },
      "source": [
        "params = model.parameters()\n",
        "\n",
        "for param in params:\n",
        "    print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1409,  0.0518,  0.3034],\n",
            "        [ 0.0913,  0.2452, -0.2616],\n",
            "        [ 0.5021,  0.0118,  0.1383],\n",
            "        [ 0.4757, -0.3128,  0.2707]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3952,  0.1285,  0.1777, -0.4675], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0391, -0.4876, -0.1731,  0.4704]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0454], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3QmDD3GgUo",
        "colab_type": "text"
      },
      "source": [
        "### Loss function（損失 / 誤差函數，或目標函數 objective function）\n",
        "常見的 `MSELoss` 和 `CrossEntropyLoss`，可傳入 tensor 計算 loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgYAMlRWIvvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "297b7790-34d4-4f91-fb94-6cf27cab7212"
      },
      "source": [
        "mse_loss_fn = nn.MSELoss()\n",
        "\n",
        "input = torch.tensor([[0., 0, 0]])\n",
        "target = torch.tensor([[1., 0, -1]])\n",
        "\n",
        "loss = mse_loss_fn(input, target)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.6667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHv1JihG5a1",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer（優化器）\n",
        "用來更新參數的方法（`SGD`、`Adagrad`、`Adam`⋯⋯）  \n",
        "在 PyTorch 中要經過 `backward()` 函數計算 gradient，  \n",
        "而在這之前要先用 `optim.zero_grad()` 將 gradient 清掉，否則 PyTorch 會將 gradient 累加起來\n",
        "\n",
        "（以下請注意 model 參數更新的方向）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7exQCz_Ivyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "fedfe39e-4c22-4875-a0cd-c81a3c8aec16"
      },
      "source": [
        "# create a simple model\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "# create a simple dataset\n",
        "X_simple = torch.tensor([[1.]])\n",
        "y_simple = torch.tensor([[2.]])\n",
        "\n",
        "# create our optimizer\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "mse_loss_fn = nn.MSELoss()\n",
        "\n",
        "y_hat = model(X_simple)\n",
        "print('model params before:', model.weight)\n",
        "loss = mse_loss_fn(y_hat, y_simple)\n",
        "optim.zero_grad()\n",
        "loss.backward()\n",
        "optim.step()\n",
        "print('model params after:', model.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model params before: Parameter containing:\n",
            "tensor([[0.1950]], requires_grad=True)\n",
            "model params after: Parameter containing:\n",
            "tensor([[0.2219]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r32NlTZIg8K",
        "colab_type": "text"
      },
      "source": [
        "## 完整 Linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvuya3mCIv1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "bb96a5cc-306f-4997-fb57-cd67426d1121"
      },
      "source": [
        "step_size = 0.1\n",
        "\n",
        "linear_module = nn.Linear(d, 1, bias=False)\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
        "\n",
        "print('iter,\\tloss,\\tw')\n",
        "\n",
        "for i in range(20):\n",
        "    y_hat = linear_module(X)\n",
        "    loss = loss_func(y_hat, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t4.37,\t[-0.5072827  0.7721884]\n",
            "1,\t2.34,\t[-0.6624694  1.0903175]\n",
            "2,\t1.25,\t[-0.77252483  1.3242052 ]\n",
            "3,\t0.67,\t[-0.8503067  1.4962891]\n",
            "4,\t0.36,\t[-0.90506    1.6230037]\n",
            "5,\t0.20,\t[-0.94342256  1.716392  ]\n",
            "6,\t0.11,\t[-0.9701522  1.7852831]\n",
            "7,\t0.06,\t[-0.98865306  1.8361537 ]\n",
            "8,\t0.04,\t[-1.0013554  1.8737577]\n",
            "9,\t0.02,\t[-1.0099901  1.9015862]\n",
            "10,\t0.02,\t[-1.0157865  1.9222052]\n",
            "11,\t0.01,\t[-1.019615   1.9375019]\n",
            "12,\t0.01,\t[-1.0220896  1.9488654]\n",
            "13,\t0.01,\t[-1.0236413  1.9573189]\n",
            "14,\t0.01,\t[-1.0245715  1.963617 ]\n",
            "15,\t0.01,\t[-1.0250894  1.9683164]\n",
            "16,\t0.01,\t[-1.0253391  1.9718288]\n",
            "17,\t0.01,\t[-1.0254192  1.9744583]\n",
            "18,\t0.01,\t[-1.0253965  1.9764304]\n",
            "19,\t0.01,\t[-1.025315   1.9779121]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-1.025315   1.9779121]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZRR9ifI7V8",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic gradient descent(SGD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H804izioIv4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "f49631ca-3e20-4b94-a925-c940f915d244"
      },
      "source": [
        "step_size = 0.01\n",
        "\n",
        "linear_module = nn.Linear(d, 1)\n",
        "loss_func = nn.MSELoss()\n",
        "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
        "print('iter,\\tloss,\\tw')\n",
        "for i in range(200):\n",
        "    rand_idx = np.random.choice(n) # take a random point from the dataset\n",
        "    x = X[rand_idx] \n",
        "    y_hat = linear_module(x)\n",
        "    loss = loss_func(y_hat, y[rand_idx]) # only compute the loss on the single point\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    if i % 20 == 0:\n",
        "        print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
        "\n",
        "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
        "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter,\tloss,\tw\n",
            "0,\t0.01,\t[-0.16747226  0.69458336]\n",
            "20,\t0.73,\t[-0.52777785  1.409119  ]\n",
            "40,\t0.05,\t[-0.7416818  1.7194623]\n",
            "60,\t0.04,\t[-0.80749375  1.8314769 ]\n",
            "80,\t0.09,\t[-0.888827   1.8813882]\n",
            "100,\t0.06,\t[-0.93712914  1.9570426 ]\n",
            "120,\t0.00,\t[-0.964763   1.9772898]\n",
            "140,\t0.00,\t[-0.9806282  1.9791763]\n",
            "160,\t0.04,\t[-0.9831248  1.9838824]\n",
            "180,\t0.01,\t[-0.9979536  1.9885796]\n",
            "\n",
            "true w\t\t [-1.  2.]\n",
            "estimated w\t [-0.9991454  1.9860797]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei4rBkksJLzc",
        "colab_type": "text"
      },
      "source": [
        "### CrossEntropyLoss \n",
        "（和 MSELoss 可以做個比較，更適合用在 classification）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcDkdfxtIv7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0256efb8-0e2d-4c05-bb28-935f7894bd7c"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "input = torch.tensor([[-1., 1],[-1, 1],[1, -1]]) # raw scores correspond to the correct class\n",
        "# input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence\n",
        "# input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class\n",
        "# input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence\n",
        "\n",
        "target = torch.tensor([1, 1, 0])\n",
        "output = loss(input, target)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1269)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufME9OsvJl3v",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Layers（卷積層）\n",
        "在處理影像時，常用到 convolutional layer 來擷取圖片特徵 （CNN） \n",
        "`nn.Conv2d` 提供 2D convolutional layer，需要三個參數\n",
        "- kernel_size：取 convolution 的 kernel 大小\n",
        "- stride：每做一次 convolution 移動的 pixel 數\n",
        "- padding：對圖片邊界做的補零（也可以補別的）\n",
        "\n",
        "以下是完整參數\n",
        "```python\n",
        "torch.nn.Conv2d(\n",
        "    in_channels,  # 輸入的 channel 數目\n",
        "    out_channels, # 輸出的 channel 數目\n",
        "    kernel_size,  # kernel 大小\n",
        "    stride=1,     # 一次移動的步數\n",
        "    padding=0,    # 邊界補齊\n",
        "    dilation=1,   \n",
        "    groups=1,\n",
        "    bias=True,    # 是否加入 bias\n",
        "    padding_mode='zeros', # 補邊的方式，預設補零\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhshHV6BKV1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# an entire mnist digit\n",
        "image = np.array([0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3803922 , 0.37647063, 0.3019608 ,0.46274513, 0.2392157 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3529412 , 0.5411765 , 0.9215687 ,0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,0.9215687 , 0.74509805, 0.08235294, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.54901963,0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.7411765 , 0.09019608, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8862746 , 0.9960785 , 0.81568635,0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,0.08235294, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.14901961, 0.32156864, 0.0509804 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.13333334,0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.32941177, 0.9960785 ,0.9960785 , 0.9176471 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.4156863 , 0.6156863 ,0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.94117653, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.26666668, 0.4666667 , 0.86274517,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.14509805, 0.73333335,0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,0.9960785 , 0.9960785 , 0.45882356, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,0.45098042, 0.34901962, 0.12156864, 0., 0.,0., 0., 0.7843138 , 0.9960785 , 0.9450981 ,0.16078432, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.6627451 , 0.9960785 ,0.6901961 , 0.24313727, 0., 0., 0.,0., 0., 0., 0., 0.18823531,0.9058824 , 0.9960785 , 0.9176471 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.07058824, 0.48627454, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.32941177, 0.9960785 , 0.9960785 ,0.6509804 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8235295 , 0.9803922 , 0.9960785 ,0.65882355, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.34901962, 0.9843138 , 0.9450981 ,0.3372549 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.01960784,0.8078432 , 0.96470594, 0.6156863 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.01568628, 0.45882356, 0.27058825,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.], dtype=np.float32)\n",
        "image_torch = torch.from_numpy(image).view(1, 1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsD3EVbqJCly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6d39eaa6-50b1-4dcf-b2d4-8f01e860c736"
      },
      "source": [
        "# a gaussian blur kernel\n",
        "gaussian_kernel = torch.tensor([[1., 2, 1],[2, 4, 2],[1, 2, 1]]) / 16.0\n",
        "\n",
        "conv = nn.Conv2d(1, 1, 3)\n",
        "# manually set the conv weight\n",
        "conv.weight.data[:] = gaussian_kernel\n",
        "\n",
        "convolved = conv(image_torch)\n",
        "\n",
        "plt.title('original image')\n",
        "plt.imshow(image_torch.view(28,28).detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "plt.title('blurred image')\n",
        "plt.imshow(convolved.view(26,26).detach().numpy())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARoElEQVR4nO3de5BW9X3H8fdH5KKCgFEJRcwab1Uz\nKZpVU7UNVmOUXNSmpZJqqGOC9ZLW6pgYM1biJBnjGB1TLwlW6yVeYsYbpNioGGvNeFuMEbzEKwq4\nsgo6oEZY4Ns/noPzgHvOsz733d/nNbOzz57vuXyfBz7POc85zzlHEYGZDX6btboBM2sOh90sEQ67\nWSIcdrNEOOxmiXDYzRLhsA8Akn4m6Zx6j1thPh2SQtLmOfWnJE2udTnWPPJxduuLpA7gZWBoRKxt\nbTdWD16ztzlJQ1rdgw0ODnsLSNpD0v2S3s42h79SVrtG0hWS5kp6Fzg4G/aDsnG+Lalb0muSvpFt\nbu9SNv0PsseTJS2RdIaknmya48vm80VJv5e0UtJiSTM/wnNYJOnQ7PFMSb+S9AtJqyQtkLSbpO9m\ny10s6bCyaY+X9Ew27kuSTtxk3kXPb7ikCyW9KmlZ9rFli4/6b5Aih73JJA0F5gB3A9sD3wJukLR7\n2WhfA34IjAIe3GT6w4HTgUOBXYDJFRb5cWA0MAE4AbhM0tis9i7wdWAM8EXgJElHVfnUvgxcD4wF\nfg/8htL/rwnAecDPy8btAb4EbA0cD1wsaZ9+Pr/zgd2ASVl9AvDvVfacFIe9+T4LjATOj4g1EXEf\n8GtgWtk4d0bE7yJifUS8v8n0U4H/ioinIuI9YGaF5fUC50VEb0TMBd4BdgeIiPsjYkG2nCeBm4DP\nVfm8/i8ifpN9vv8VsF32HHuBm4EOSWOy5f53RLwYJf9L6Y3vryo9P0kCZgD/FhErImIV8CPgmCp7\nTkqfe1qtof4MWBwR68uGvUJpDbXB4grTd/VzXIDlm+xge4/Smw2S9qe0pvwUMAwYTimo1VhW9vhP\nwJsRsa7sb7Llvi3pCOBcSmvozYAtgQXZOEXPb7ts3Pml3AMgwPs1+sFr9uZ7DZgoqfy13xFYWvZ3\n0SGSbmCHsr8n1tDLjcBsYGJEjAZ+Rik8DSNpOHArcCEwLiLGAHPLllv0/N6k9MaxV0SMyX5GR8TI\nRvY8WDjszfcIpbXrtyUNzY5Vf5nSpm5/3AIcn+3k2xKo5Zj6KGBFRLwvaT9K+woabcMWxBvA2mwt\nf1hZPff5ZVtDV1L6jL89gKQJkr7QhL4HPIe9ySJiDaVwH0FpTXU58PWIeLaf098F/BT4LfAC8HBW\nWl1FOycD50laRWkn1y1VzOMjyT5n/0u2rLcovcHMLqtXen7f2TBc0krgXrJ9EFbMX6oZ4CTtASwE\nhg/GL78M9ufXTF6zD0CSjs6ON48FfgzMGUxBGOzPr1Uc9oHpRErHql8E1gEntbaduhvsz68lvBlv\nlgiv2c0S0dQv1QzT8BjBVs1cpFlS3udd1sTqPr8rUVPYs+8xX0LpG0z/GRHnF40/gq3YX4fUskgz\nK/BIzMutVb0Zn516eRml48V7AtMk7Vnt/MyssWr5zL4f8EJEvJR9UeRm4Mj6tGVm9VZL2Cew8UkK\nS9j4ZA4AJM2Q1CWpq7eqL3mZWT00fG98RMyKiM6I6BzK8EYvzsxy1BL2pWx8RtIObHzmlpm1kVrC\n/hiwq6SdJA2jdAGB2RWmMbMWqfrQW0SslXQqpcsPDQGujoin6taZmdVVTcfZs8scza1TL2bWQP66\nrFkiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uE\nw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S\n4bCbJcJhN0tETbdslrQIWAWsA9ZGRGc9mjKz+qsp7JmDI+LNOszHzBrIm/Fmiag17AHcLWm+pBl9\njSBphqQuSV29rK5xcWZWrVo34w+KiKWStgfukfRsRDxQPkJEzAJmAWytbaLG5ZlZlWpas0fE0ux3\nD3A7sF89mjKz+qs67JK2kjRqw2PgMGBhvRozs/qqZTN+HHC7pA3zuTEi/qcuXZlZ3VUd9oh4CfiL\nOvZiZg3kQ29miXDYzRLhsJslwmE3S4TDbpaIepwIYy3WffoBuTVV+M7iiOXFI7z158XTj39oXfH8\n5zxaPANrGq/ZzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEDJrj7D2n5B9rBnj7072F9dsPu7Se\n7TTVHsMeq3ra92NtYX30ZlsU1nuOe7ew/tpP8/+LXfT65wunXT5168L62sVLCuu2Ma/ZzRLhsJsl\nwmE3S4TDbpYIh90sEQ67WSIcdrNEKKJ5N2nZWtvE/jqk6umfu3Lf3NqzUy4vnHa4hla9XGuNYxdN\nLqy/9bUKx+EXvVrHbgaGR2IeK2OF+qp5zW6WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJWJAnc9+\nxcHX5dYqHUf/8fJdC+s9a0ZV1VM93Db/M4X1Hef0edi0LSw5pHh9ccGUG3NrXx25snDaX3TcX1g/\n9sbJhfW3/mGH3FqK58JXXLNLulpSj6SFZcO2kXSPpOez32Mb26aZ1ao/m/HXAIdvMuwsYF5E7ArM\ny/42szZWMewR8QCwYpPBRwLXZo+vBY6qc19mVmfVfmYfFxHd2ePXgXF5I0qaAcwAGMGWVS7OzGpV\n8974KJ1Jk3s2TUTMiojOiOgcyvBaF2dmVao27MskjQfIfvfUryUza4Rqwz4bmJ49ng7cWZ92zKxR\nKp7PLukmYDKwLbAMOBe4A7gF2BF4BZgaEZvuxPuQWs9n12f2yq29Oan43Obt7/hjYX3d8ortWxU2\n+3T+Dd6/dPPvCqc9Zczimpa9+1Un5dY6znmopnm3q6Lz2SvuoIuIaTml6lNrZk3nr8uaJcJhN0uE\nw26WCIfdLBEOu1kiBtSlpG1wWf7Nvyysd33/iprmP3/1mtza2TvtV9O825UvJW1mDrtZKhx2s0Q4\n7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLxIC6ZbMNPEvO\nPiC3tn7vVQ1d9rgh+eezr/2b4ttkb37f/Hq303Jes5slwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxm\nifB14weBzT/ZkVt74YTxhdNefsysOnezsckjenNrQ9S6dc2Lve8U1k/+xEFN6qS+arpuvKSrJfVI\nWlg2bKakpZKeyH6m1LNhM6u//ry1XgMc3sfwiyNiUvYzt75tmVm9VQx7RDwArGhCL2bWQLV8aDpV\n0pPZZv7YvJEkzZDUJamrl9U1LM7MalFt2K8AdgYmAd3AT/JGjIhZEdEZEZ1DGV7l4sysVlWFPSKW\nRcS6iFgPXAkMzltimg0iVYVdUvnxnKOBhXnjmll7qHg+u6SbgMnAtpKWAOcCkyVNAgJYBJzYwB4H\nvXf+fv/C+hv7FL8nn/e3N+fWjhn1VlU91U97fm/r0HtPK6zvRleTOmmeimGPiGl9DL6qAb2YWQO1\n59uumdWdw26WCIfdLBEOu1kiHHazRPhS0nWgvfcqrI+5tLuwPrfjisJ6I08FvePdkYX1hX/aoab5\n//qCybm1IauLT6+eft6cwvqM0a9V0xIAw14fWvW0A5XX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwR\nDrtZInycvZ9e+X7+rYfPOeaXhdP+46jlhfVX175XWH92Te5VvwD41k3fyK1t2d3nVYU/MP7+Nwvr\n655+rrBeyWgernra5787rsLMi4+zv1xwueiOO4svJT0Yec1ulgiH3SwRDrtZIhx2s0Q47GaJcNjN\nEuGwmyXCx9n7acy+Pbm1SsfRD3n6K4X13v/4eGF9izsfLax38FBhvci6qqes3frP7V1YP2pMpYsY\nF6+rVqwfll98dEGFeQ8+XrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonozy2bJwLXAeMo3aJ5\nVkRcImkb4JdAB6XbNk+NiFbfH7hhPnZC/vnPu5x+UuG0O59ZfBx8c16tqqeB7q3dRhTWDxxR27po\nxsJjc2vbUtt5+gNRf17NtcAZEbEn8FngFEl7AmcB8yJiV2Be9reZtamKYY+I7oh4PHu8CngGmAAc\nCVybjXYtcFSjmjSz2n2k7SRJHcDewCPAuIjYcF+j1ylt5ptZm+p32CWNBG4FTouIleW1iAhKn+f7\nmm6GpC5JXb2srqlZM6tev8IuaSiloN8QEbdlg5dJGp/VxwN9nikSEbMiojMiOocyvB49m1kVKoZd\nkoCrgGci4qKy0mxgevZ4OnBn/dszs3rpzymuBwLHAQskPZENOxs4H7hF0gnAK8DUxrTYHtZ2v55b\n2/nM/JrlW77v2pqmf2ZN8SW4R10+uqb5DzYVwx4RDwJ5Fx8/pL7tmFmj+Bt0Zolw2M0S4bCbJcJh\nN0uEw26WCIfdLBG+lLQ11BcWrsyt3T7msgpTF1wKGpj+1PTC+ti7Hqsw/7R4zW6WCIfdLBEOu1ki\nHHazRDjsZolw2M0S4bCbJcLH2a2h/m7rJ3NrW242snDa53rfLaxveemYqnpKldfsZolw2M0S4bCb\nJcJhN0uEw26WCIfdLBEOu1kifJzdatJz8gGF9XFD8s8pf7k3/zbYANN+dGZhfdu7im+FbRvzmt0s\nEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S0TF4+ySJgLXAeOAAGZFxCWSZgLfBN7IRj07IuY2qlFr\nDQ0fXlj/6j/fV1hftX5Nbm3KoycVTrvjz30cvZ7686WatcAZEfG4pFHAfEn3ZLWLI+LCxrVnZvVS\nMewR0Q10Z49XSXoGmNDoxsysvj7SZ3ZJHcDewCPZoFMlPSnpakljc6aZIalLUlcvq2tq1syq1++w\nSxoJ3AqcFhErgSuAnYFJlNb8P+lruoiYFRGdEdE5lOLPf2bWOP0Ku6ShlIJ+Q0TcBhARyyJiXUSs\nB64E9mtcm2ZWq4phlyTgKuCZiLiobPj4stGOBhbWvz0zq5f+7I0/EDgOWCDpiWzY2cA0SZMoHY5b\nBJzYkA6ttdZHYfn6OQcX1u/6w+Tc2o63PFxNR1al/uyNfxBQHyUfUzcbQPwNOrNEOOxmiXDYzRLh\nsJslwmE3S4TDbpYIX0raCkVv/imqAB3f82moA4XX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZ\nIhRRfL5yXRcmvQG8UjZoW+DNpjXw0bRrb+3aF7i3atWzt09ExHZ9FZoa9g8tXOqKiM6WNVCgXXtr\n177AvVWrWb15M94sEQ67WSJaHfZZLV5+kXbtrV37AvdWrab01tLP7GbWPK1es5tZkzjsZoloSdgl\nHS7pj5JekHRWK3rII2mRpAWSnpDU1eJerpbUI2lh2bBtJN0j6fnsd5/32GtRbzMlLc1euyckTWlR\nbxMl/VbS05KekvSv2fCWvnYFfTXldWv6Z3ZJQ4DngM8DS4DHgGkR8XRTG8khaRHQGREt/wKGpL8G\n3gGui4hPZcMuAFZExPnZG+XYiPhOm/Q2E3in1bfxzu5WNL78NuPAUcA/0cLXrqCvqTThdWvFmn0/\n4IWIeCki1gA3A0e2oI+2FxEPACs2GXwkcG32+FpK/1maLqe3thAR3RHxePZ4FbDhNuMtfe0K+mqK\nVoR9ArC47O8ltNf93gO4W9J8STNa3UwfxkVEd/b4dWBcK5vpQ8XbeDfTJrcZb5vXrprbn9fKO+g+\n7KCI2Ac4Ajgl21xtS1H6DNZOx077dRvvZunjNuMfaOVrV+3tz2vVirAvBSaW/b1DNqwtRMTS7HcP\ncDvtdyvqZRvuoJv97mlxPx9op9t493WbcdrgtWvl7c9bEfbHgF0l7SRpGHAMMLsFfXyIpK2yHSdI\n2go4jPa7FfVsYHr2eDpwZwt72Ui73MY77zbjtPi1a/ntzyOi6T/AFEp75F8EvteKHnL6+iTwh+zn\nqVb3BtxEabOul9K+jROAjwHzgOeBe4Ft2qi364EFwJOUgjW+Rb0dRGkT/UngiexnSqtfu4K+mvK6\n+euyZonwDjqzRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBH/D3ImkM6hEnS6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAEICAYAAACUHfLiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n00Qz2rY31PcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HahIXYdIJCq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "c808859a-1eb9-43f0-e36e-4552d6a3732d"
      },
      "source": [
        "im_channels = 3 # if we are working with RGB images, there are 3 input channels, with black and white, 1\n",
        "out_channels = 16 # this is a hyperparameter we can tune\n",
        "kernel_size = 3 # this is another hyperparameter we can tune\n",
        "batch_size = 4\n",
        "image_width = 32\n",
        "image_height = 32\n",
        "\n",
        "im = torch.randn(batch_size, im_channels, image_width, image_height)\n",
        "\n",
        "m = nn.Conv2d(im_channels, out_channels, kernel_size)\n",
        "convolved = m(im) # it is a module so we can call it\n",
        "\n",
        "print('im shape', im.shape)\n",
        "print('convolved im shape', convolved.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im shape torch.Size([4, 3, 32, 32])\n",
            "convolved im shape torch.Size([4, 16, 30, 30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zNif2F6OM7_",
        "colab_type": "text"
      },
      "source": [
        "### Learning rate schedulers\n",
        "可以對 learning rate 的更新方式進行客製化自訂\n",
        "\n",
        "詳見 [PyTorch 官方文件](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sexuJDZBLvWx",
        "colab_type": "text"
      },
      "source": [
        "## Dataset 與 Dataloader\n",
        "在處理訓練資料時，進行資料型態的前處理與分批（batch）等是相當麻煩的事。  \n",
        "PyTorch 提供了一個很好的 dataset 與 dataloader 讓我們進行分裝以利訓練進行，還可以依需求自訂 dataset 的型態  \n",
        "\n",
        "簡言之，`dataset` 是用來做打包與預處理（例如輸入資料路徑自動讀取）；  \n",
        "`Dataloader` 則是可以將整個資料集（dataset）按照 batch 進行迭代分裝或 shuffle（會得到一個 iterator 以利 for 迴圈讀取）\n",
        "\n",
        "其中 `dataset` 必須給予 `__len__`（dataset 大小）與`__getitem__`（取得特定 index 的資料）的定義  \n",
        "（否則會跳出 `NotImplementedError`）\n",
        "\n",
        "另外 `Dataloader` 可以自訂 `collate_fn` 決定 batch 的分裝方式\n",
        "\n",
        "以下取自 PyTorch 官方（只節錄需要 demo 的部分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i00uDZriJCwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FaceLandmarksDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        landmarks = np.array([landmarks])\n",
        "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpi0KA1wLfCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "569ee08e-62b4-4ed3-c5a3-59d79a140219"
      },
      "source": [
        "# 處理圖片\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "\n",
        "        # h and w are swapped for landmarks because for images,\n",
        "        # x and y axes are axis 1 and 0 respectively\n",
        "        landmarks = landmarks * [new_w / w, new_h / h]\n",
        "\n",
        "        return {'image': img, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        landmarks = landmarks - [left, top]\n",
        "\n",
        "        return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, landmarks = sample['image'], sample['landmarks']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'landmarks': torch.from_numpy(landmarks)}\n",
        "\n",
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                           root_dir='data/faces/',\n",
        "                                           transform=transforms.Compose([\n",
        "                                               Rescale(256),\n",
        "                                               RandomCrop(224),\n",
        "                                               ToTensor()\n",
        "                                           ]))\n",
        "\n",
        "for i in range(len(transformed_dataset)):\n",
        "    sample = transformed_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].size(), sample['landmarks'].size())\n",
        "\n",
        "    if i == 3:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "1 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "2 torch.Size([3, 224, 224]) torch.Size([68, 2])\n",
            "3 torch.Size([3, 224, 224]) torch.Size([68, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h4RprFzJC14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "d51ef288-2a53-455a-80b0-68ab93a0326e"
      },
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=4)\n",
        "\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "    print(i_batch, sample_batched['image'].size(),\n",
        "          sample_batched['landmarks'].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "4 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "5 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "6 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "7 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "8 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "9 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "10 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "11 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "12 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "13 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "14 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "15 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "16 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n",
            "17 torch.Size([1, 3, 224, 224]) torch.Size([1, 68, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8onygpVPbSF",
        "colab_type": "text"
      },
      "source": [
        "# 好用的參考連結\n",
        "- [投影片原始檔案](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML2020/PyTorch_Introduction.slides.html)  \n",
        "- [60-minute PyTorch Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)  \n",
        "- [PyTorch 官方文檔](https://pytorch.org/docs/stable/index.html)  \n",
        "- [Lecture notes on Auto-Diff](https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI9xvCDQQU38",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown # [補充] 混合精度訓練（Mixed Precision Training）\n",
        "#@markdown \n",
        "#@markdown 可以加速並更有效率的訓練模型，只需稍加改動 code  \n",
        "#@markdown ### references\n",
        "#@markdown - https://liangtaiwan.github.io  \n",
        "#@markdown - https://github.com/NVIDIA/apex\n",
        "\n",
        "#@markdown ### Run me to install APEX...\n",
        "\n",
        "%%bash\n",
        "# echo \"Check CUDA Version 9 or newer\"\n",
        "# nvcc --version\n",
        "\n",
        "# echo\n",
        "CHECK_APEX=`echo -e \"try: import apex\\nexcept: print(0)\"`\n",
        "if [[ `python3 -c \"$CHECK_APEX\"` = \"0\" ]]; then\n",
        "    # echo -e \"Install APEX...\"\n",
        "    git clone https://github.com/NVIDIA/apex -q\n",
        "    cd apex\n",
        "    pip install -q -v --no-cache-dir ./\n",
        "    rm -rf apex\n",
        "    # echo \"DONE!\"\n",
        "# else\n",
        "    # echo \"APEX installed!\";\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgvu4LyYJDBC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ed980f9-b8ca-42f4-ba0d-cfeec3ef4ae9"
      },
      "source": [
        "#@markdown ```python\n",
        "#@markdown # Declare model and optimizer as usual, with default (FP32) precision\n",
        "#@markdown model = torch.nn.Linear(D_in, D_out).cuda()\n",
        "#@markdown optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "#@markdown \n",
        "#@markdown # Allow Amp to perform casts as required by the opt_level\n",
        "#@markdown model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "#@markdown ...\n",
        "#@markdown # loss.backward() becomes:\n",
        "#@markdown with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "#@markdown     scaled_loss.backward()\n",
        "#@markdown ...\n",
        "#@markdown ```\n",
        "\n",
        "import apex.amp as amp\n",
        "\n",
        "# %%\n",
        "D_in, D_out = 2000, 100\n",
        "\n",
        "# Declare model and optimizer as usual, with default (FP32) precision\n",
        "model = torch.nn.Linear(D_in, D_out).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "# %%\n",
        "input_data = torch.rand(128, 2000).cuda()\n",
        "output_ans = torch.rand(128, 100).cuda()\n",
        "\n",
        "# %%\n",
        "# Allow Amp to perform casts as required by the opt_level\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "# ...\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# %%\n",
        "for i in range(200):\n",
        "    output_guess = model(input_data)\n",
        "    loss = criterion(output_guess, output_ans)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss.backward() becomes:\n",
        "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {i + 1:3d}: loss = {loss.item():6.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
            "Epoch   1: loss = 0.4835\n",
            "Epoch   2: loss = 0.4760\n",
            "Epoch   3: loss = 0.4687\n",
            "Epoch   4: loss = 0.4616\n",
            "Epoch   5: loss = 0.4546\n",
            "Epoch   6: loss = 0.4477\n",
            "Epoch   7: loss = 0.4409\n",
            "Epoch   8: loss = 0.4343\n",
            "Epoch   9: loss = 0.4279\n",
            "Epoch  10: loss = 0.4215\n",
            "Epoch  11: loss = 0.4153\n",
            "Epoch  12: loss = 0.4092\n",
            "Epoch  13: loss = 0.4033\n",
            "Epoch  14: loss = 0.3974\n",
            "Epoch  15: loss = 0.3917\n",
            "Epoch  16: loss = 0.3861\n",
            "Epoch  17: loss = 0.3806\n",
            "Epoch  18: loss = 0.3752\n",
            "Epoch  19: loss = 0.3699\n",
            "Epoch  20: loss = 0.3647\n",
            "Epoch  21: loss = 0.3596\n",
            "Epoch  22: loss = 0.3547\n",
            "Epoch  23: loss = 0.3498\n",
            "Epoch  24: loss = 0.3450\n",
            "Epoch  25: loss = 0.3403\n",
            "Epoch  26: loss = 0.3357\n",
            "Epoch  27: loss = 0.3312\n",
            "Epoch  28: loss = 0.3268\n",
            "Epoch  29: loss = 0.3225\n",
            "Epoch  30: loss = 0.3183\n",
            "Epoch  31: loss = 0.3141\n",
            "Epoch  32: loss = 0.3101\n",
            "Epoch  33: loss = 0.3061\n",
            "Epoch  34: loss = 0.3022\n",
            "Epoch  35: loss = 0.2984\n",
            "Epoch  36: loss = 0.2946\n",
            "Epoch  37: loss = 0.2909\n",
            "Epoch  38: loss = 0.2873\n",
            "Epoch  39: loss = 0.2838\n",
            "Epoch  40: loss = 0.2803\n",
            "Epoch  41: loss = 0.2770\n",
            "Epoch  42: loss = 0.2736\n",
            "Epoch  43: loss = 0.2704\n",
            "Epoch  44: loss = 0.2672\n",
            "Epoch  45: loss = 0.2641\n",
            "Epoch  46: loss = 0.2610\n",
            "Epoch  47: loss = 0.2580\n",
            "Epoch  48: loss = 0.2551\n",
            "Epoch  49: loss = 0.2522\n",
            "Epoch  50: loss = 0.2494\n",
            "Epoch  51: loss = 0.2466\n",
            "Epoch  52: loss = 0.2439\n",
            "Epoch  53: loss = 0.2412\n",
            "Epoch  54: loss = 0.2386\n",
            "Epoch  55: loss = 0.2361\n",
            "Epoch  56: loss = 0.2336\n",
            "Epoch  57: loss = 0.2311\n",
            "Epoch  58: loss = 0.2287\n",
            "Epoch  59: loss = 0.2263\n",
            "Epoch  60: loss = 0.2240\n",
            "Epoch  61: loss = 0.2218\n",
            "Epoch  62: loss = 0.2196\n",
            "Epoch  63: loss = 0.2174\n",
            "Epoch  64: loss = 0.2153\n",
            "Epoch  65: loss = 0.2132\n",
            "Epoch  66: loss = 0.2111\n",
            "Epoch  67: loss = 0.2091\n",
            "Epoch  68: loss = 0.2072\n",
            "Epoch  69: loss = 0.2052\n",
            "Epoch  70: loss = 0.2033\n",
            "Epoch  71: loss = 0.2015\n",
            "Epoch  72: loss = 0.1997\n",
            "Epoch  73: loss = 0.1979\n",
            "Epoch  74: loss = 0.1962\n",
            "Epoch  75: loss = 0.1945\n",
            "Epoch  76: loss = 0.1928\n",
            "Epoch  77: loss = 0.1912\n",
            "Epoch  78: loss = 0.1896\n",
            "Epoch  79: loss = 0.1880\n",
            "Epoch  80: loss = 0.1864\n",
            "Epoch  81: loss = 0.1849\n",
            "Epoch  82: loss = 0.1835\n",
            "Epoch  83: loss = 0.1820\n",
            "Epoch  84: loss = 0.1806\n",
            "Epoch  85: loss = 0.1792\n",
            "Epoch  86: loss = 0.1778\n",
            "Epoch  87: loss = 0.1765\n",
            "Epoch  88: loss = 0.1752\n",
            "Epoch  89: loss = 0.1739\n",
            "Epoch  90: loss = 0.1726\n",
            "Epoch  91: loss = 0.1714\n",
            "Epoch  92: loss = 0.1702\n",
            "Epoch  93: loss = 0.1690\n",
            "Epoch  94: loss = 0.1678\n",
            "Epoch  95: loss = 0.1667\n",
            "Epoch  96: loss = 0.1656\n",
            "Epoch  97: loss = 0.1645\n",
            "Epoch  98: loss = 0.1634\n",
            "Epoch  99: loss = 0.1623\n",
            "Epoch 100: loss = 0.1613\n",
            "Epoch 101: loss = 0.1603\n",
            "Epoch 102: loss = 0.1593\n",
            "Epoch 103: loss = 0.1583\n",
            "Epoch 104: loss = 0.1574\n",
            "Epoch 105: loss = 0.1565\n",
            "Epoch 106: loss = 0.1555\n",
            "Epoch 107: loss = 0.1547\n",
            "Epoch 108: loss = 0.1538\n",
            "Epoch 109: loss = 0.1529\n",
            "Epoch 110: loss = 0.1521\n",
            "Epoch 111: loss = 0.1512\n",
            "Epoch 112: loss = 0.1504\n",
            "Epoch 113: loss = 0.1496\n",
            "Epoch 114: loss = 0.1489\n",
            "Epoch 115: loss = 0.1481\n",
            "Epoch 116: loss = 0.1474\n",
            "Epoch 117: loss = 0.1466\n",
            "Epoch 118: loss = 0.1459\n",
            "Epoch 119: loss = 0.1452\n",
            "Epoch 120: loss = 0.1445\n",
            "Epoch 121: loss = 0.1438\n",
            "Epoch 122: loss = 0.1432\n",
            "Epoch 123: loss = 0.1425\n",
            "Epoch 124: loss = 0.1419\n",
            "Epoch 125: loss = 0.1413\n",
            "Epoch 126: loss = 0.1407\n",
            "Epoch 127: loss = 0.1401\n",
            "Epoch 128: loss = 0.1395\n",
            "Epoch 129: loss = 0.1389\n",
            "Epoch 130: loss = 0.1383\n",
            "Epoch 131: loss = 0.1378\n",
            "Epoch 132: loss = 0.1372\n",
            "Epoch 133: loss = 0.1367\n",
            "Epoch 134: loss = 0.1362\n",
            "Epoch 135: loss = 0.1357\n",
            "Epoch 136: loss = 0.1352\n",
            "Epoch 137: loss = 0.1347\n",
            "Epoch 138: loss = 0.1342\n",
            "Epoch 139: loss = 0.1337\n",
            "Epoch 140: loss = 0.1333\n",
            "Epoch 141: loss = 0.1328\n",
            "Epoch 142: loss = 0.1324\n",
            "Epoch 143: loss = 0.1319\n",
            "Epoch 144: loss = 0.1315\n",
            "Epoch 145: loss = 0.1311\n",
            "Epoch 146: loss = 0.1307\n",
            "Epoch 147: loss = 0.1303\n",
            "Epoch 148: loss = 0.1299\n",
            "Epoch 149: loss = 0.1295\n",
            "Epoch 150: loss = 0.1291\n",
            "Epoch 151: loss = 0.1287\n",
            "Epoch 152: loss = 0.1284\n",
            "Epoch 153: loss = 0.1280\n",
            "Epoch 154: loss = 0.1277\n",
            "Epoch 155: loss = 0.1273\n",
            "Epoch 156: loss = 0.1270\n",
            "Epoch 157: loss = 0.1267\n",
            "Epoch 158: loss = 0.1263\n",
            "Epoch 159: loss = 0.1260\n",
            "Epoch 160: loss = 0.1257\n",
            "Epoch 161: loss = 0.1254\n",
            "Epoch 162: loss = 0.1251\n",
            "Epoch 163: loss = 0.1248\n",
            "Epoch 164: loss = 0.1245\n",
            "Epoch 165: loss = 0.1243\n",
            "Epoch 166: loss = 0.1240\n",
            "Epoch 167: loss = 0.1237\n",
            "Epoch 168: loss = 0.1235\n",
            "Epoch 169: loss = 0.1232\n",
            "Epoch 170: loss = 0.1229\n",
            "Epoch 171: loss = 0.1227\n",
            "Epoch 172: loss = 0.1224\n",
            "Epoch 173: loss = 0.1222\n",
            "Epoch 174: loss = 0.1220\n",
            "Epoch 175: loss = 0.1217\n",
            "Epoch 176: loss = 0.1215\n",
            "Epoch 177: loss = 0.1213\n",
            "Epoch 178: loss = 0.1211\n",
            "Epoch 179: loss = 0.1209\n",
            "Epoch 180: loss = 0.1207\n",
            "Epoch 181: loss = 0.1205\n",
            "Epoch 182: loss = 0.1202\n",
            "Epoch 183: loss = 0.1201\n",
            "Epoch 184: loss = 0.1199\n",
            "Epoch 185: loss = 0.1197\n",
            "Epoch 186: loss = 0.1195\n",
            "Epoch 187: loss = 0.1193\n",
            "Epoch 188: loss = 0.1191\n",
            "Epoch 189: loss = 0.1189\n",
            "Epoch 190: loss = 0.1188\n",
            "Epoch 191: loss = 0.1186\n",
            "Epoch 192: loss = 0.1184\n",
            "Epoch 193: loss = 0.1183\n",
            "Epoch 194: loss = 0.1181\n",
            "Epoch 195: loss = 0.1180\n",
            "Epoch 196: loss = 0.1178\n",
            "Epoch 197: loss = 0.1177\n",
            "Epoch 198: loss = 0.1175\n",
            "Epoch 199: loss = 0.1174\n",
            "Epoch 200: loss = 0.1172\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}